"""
Stable Depth Transformer (SDT) Head

Adapted from AnythingDepth for integration with DA3-Serverless.
Original source: ext_code/AnyDepth/model/sdt_head.py

This module provides:
- DySample: Dynamic upsampling with learnable offsets
- WeightedFusion: Multi-scale feature fusion with learnable weights
- SpatialDetailEnhancer: Depthwise convolution for detail enhancement
- DySampleUpsamplerWrapper: Two-stage 4x upsampling
- SDTHead: Main head combining all components
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List


def normal_init(module, mean=0, std=1, bias=0):
    """Initialize module weights with normal distribution."""
    if hasattr(module, 'weight') and module.weight is not None:
        nn.init.normal_(module.weight, mean, std)
    if hasattr(module, 'bias') and module.bias is not None:
        nn.init.constant_(module.bias, bias)


def constant_init(module, val, bias=0):
    """Initialize module weights with constant value."""
    if hasattr(module, 'weight') and module.weight is not None:
        nn.init.constant_(module.weight, val)
    if hasattr(module, 'bias') and module.bias is not None:
        nn.init.constant_(module.bias, bias)


class DySample(nn.Module):
    """
    Dynamic Upsampling module.

    Learns content-aware sampling offsets for better upsampling quality
    compared to bilinear interpolation.

    Args:
        in_channels: Number of input channels
        scale: Upsampling scale factor (default: 2)
        style: Sampling style - 'lp' (learn-then-pixel) or 'pl' (pixel-then-learn)
        groups: Number of groups for offset computation
        dyscope: Whether to use dynamic scope scaling
    """

    def __init__(self, in_channels, scale=2, style='lp', groups=4, dyscope=False):
        super().__init__()
        self.scale = scale
        self.style = style
        self.groups = groups
        assert style in ['lp', 'pl']
        if style == 'pl':
            assert in_channels >= scale ** 2 and in_channels % scale ** 2 == 0
        assert in_channels >= groups and in_channels % groups == 0

        if style == 'pl':
            in_channels = in_channels // scale ** 2
            out_channels = 2 * groups
        else:
            out_channels = 2 * groups * scale ** 2

        self.offset = nn.Conv2d(in_channels, out_channels, 1)
        normal_init(self.offset, std=0.001)
        if dyscope:
            self.scope = nn.Conv2d(in_channels, out_channels, 1, bias=False)
            constant_init(self.scope, val=0.)

        self.register_buffer('init_pos', self._init_pos())

    def _init_pos(self):
        """Initialize position grid for sampling."""
        h = torch.arange((-self.scale + 1) / 2, (self.scale - 1) / 2 + 1) / self.scale
        return torch.stack(torch.meshgrid([h, h], indexing='ij')).transpose(1, 2).repeat(1, self.groups, 1).reshape(1, -1, 1, 1)

    def sample(self, x, offset):
        """Sample from input using computed offsets."""
        B, _, H, W = offset.shape
        offset = offset.reshape(B, 2, -1, H, W)
        coords_h = torch.arange(H, device=x.device, dtype=x.dtype) + 0.5
        coords_w = torch.arange(W, device=x.device, dtype=x.dtype) + 0.5
        coords = torch.stack(torch.meshgrid([coords_w, coords_h], indexing='ij')).transpose(1, 2).unsqueeze(1).unsqueeze(0)
        normalizer = torch.tensor([W, H], dtype=x.dtype, device=x.device).reshape(1, 2, 1, 1, 1)
        coords = 2 * (coords + offset) / normalizer - 1
        coords = F.pixel_shuffle(coords.reshape(B, -1, H, W), self.scale).reshape(
            B, 2, -1, self.scale * H, self.scale * W).permute(0, 2, 3, 4, 1).contiguous().flatten(0, 1)
        return F.grid_sample(x.reshape(B * self.groups, -1, H, W), coords, mode='bilinear',
                             align_corners=False, padding_mode="border").reshape(B, -1, self.scale * H, self.scale * W)

    def forward_lp(self, x):
        """Forward pass for learn-then-pixel style."""
        if hasattr(self, 'scope'):
            offset = self.offset(x) * self.scope(x).sigmoid() * 0.5 + self.init_pos
        else:
            offset = self.offset(x) * 0.25 + self.init_pos
        return self.sample(x, offset)

    def forward_pl(self, x):
        """Forward pass for pixel-then-learn style."""
        x_ = F.pixel_shuffle(x, self.scale)
        if hasattr(self, 'scope'):
            offset = F.pixel_unshuffle(self.offset(x_) * self.scope(x_).sigmoid(), self.scale) * 0.5 + self.init_pos
        else:
            offset = F.pixel_unshuffle(self.offset(x_), self.scale) * 0.25 + self.init_pos
        return self.sample(x, offset)

    def forward(self, x):
        """Forward pass - dispatches to appropriate style."""
        if self.style == 'pl':
            return self.forward_pl(x)
        return self.forward_lp(x)


class WeightedFusion(nn.Module):
    """
    Multi-scale feature fusion with learnable weights.

    Fuses features from multiple backbone layers using learned
    importance weights via softmax normalization.

    Args:
        in_channels: List of input channel dimensions for each layer
        out_channels: Output channel dimension after fusion
    """

    def __init__(self, in_channels: List[int], out_channels: int = 256):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels

        self.projections = nn.ModuleList([
            nn.Sequential(nn.Linear(in_dim, out_channels, bias=False), nn.GELU())
            for in_dim in in_channels
        ])

        self.readout_projects = nn.ModuleList([
            nn.Sequential(nn.Linear(2 * in_dim, in_dim), nn.GELU())
            for in_dim in in_channels
        ])

        self.layer_weights = nn.Parameter(torch.ones(len(in_channels)))

    def forward(self, features: List[torch.Tensor]) -> torch.Tensor:
        """
        Fuse multi-scale features.

        Args:
            features: List of feature tensors, each either:
                - tuple (spatial_tensor [B,C,H,W], cls_token [B,C])
                - tensor [B,C,H,W] or [B,N,C]

        Returns:
            Fused feature tokens [B, N, out_channels]
        """
        assert len(features) == len(self.projections)
        projected_layer_tokens = []

        for i, layer_feature in enumerate(features):
            if isinstance(layer_feature, tuple):
                spatial_tensor, cls_token = layer_feature
                B, C, H, W = spatial_tensor.shape
                spatial_tokens = spatial_tensor.flatten(2).permute(0, 2, 1).contiguous()
                cls_token_expanded = cls_token.unsqueeze(1).expand_as(spatial_tokens)
                tokens_with_cls = torch.cat((spatial_tokens, cls_token_expanded), dim=-1)
                enhanced_tokens = self.readout_projects[i](tokens_with_cls)
                projected_tokens = self.projections[i](enhanced_tokens)
            else:
                if layer_feature.dim() == 4:
                    B, C, H, W = layer_feature.shape
                    layer_tokens = layer_feature.flatten(2).permute(0, 2, 1).contiguous()
                else:
                    layer_tokens = layer_feature
                projected_tokens = self.projections[i](layer_tokens)

            projected_layer_tokens.append(projected_tokens)

        layer_weights = F.softmax(self.layer_weights, dim=0)
        fused_tokens = torch.zeros_like(projected_layer_tokens[0])
        for i, projected_tokens in enumerate(projected_layer_tokens):
            fused_tokens = fused_tokens + layer_weights[i] * projected_tokens

        return fused_tokens


class SpatialDetailEnhancer(nn.Module):
    """
    Spatial detail enhancement using depthwise convolution.

    Applies depthwise separable convolution with residual connection
    to enhance spatial details in feature maps.

    Args:
        channels: Number of input/output channels
    """

    def __init__(self, channels: int):
        super().__init__()
        self.dwconv = nn.Conv2d(channels, channels, kernel_size=3, padding=1, groups=channels, bias=False)
        self.norm = nn.BatchNorm2d(channels)
        self.activation = nn.ReLU(inplace=True)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Apply detail enhancement with residual connection."""
        residual = x
        x = self.dwconv(x)
        x = self.norm(x)
        x = self.activation(x + residual)
        return x


class DySampleUpsamplerWrapper(nn.Module):
    """
    Two-stage dynamic upsampling wrapper.

    Applies DySample upsampling twice (2x each = 4x total) with
    intermediate convolution and normalization.

    Args:
        feature_dim: Feature dimension
        scale_factor: Total upsampling factor (must be 4)
        style: DySample style ('lp' or 'pl')
        groups: Number of groups for DySample
        dyscope: Whether to use dynamic scope
    """

    def __init__(self, feature_dim: int, scale_factor: int = 4, style: str = 'lp', groups: int = 4, dyscope: bool = False):
        super().__init__()
        self.scale_factor = scale_factor
        self.feature_dim = feature_dim

        self.dysample1 = nn.Sequential(
            DySample(feature_dim, scale=2, style=style, groups=groups, dyscope=dyscope),
            nn.Conv2d(feature_dim, feature_dim, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(feature_dim),
            nn.ReLU(inplace=True))
        self.dysample2 = nn.Sequential(
            DySample(feature_dim, scale=2, style=style, groups=groups, dyscope=dyscope),
            nn.Conv2d(feature_dim, feature_dim, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(feature_dim),
            nn.ReLU(inplace=True))

    def forward(self, features: torch.Tensor, target_size: tuple = None) -> torch.Tensor:
        """Apply two-stage upsampling."""
        x = self.dysample1(features)
        x = self.dysample2(x)
        return x


class SDTHead(nn.Module):
    """
    Stable Depth Transformer Head.

    Combines weighted multi-scale fusion, spatial detail enhancement,
    and dynamic upsampling for high-quality depth prediction.

    Args:
        in_channels: List of 4 input channel dimensions from backbone layers
        fusion_channels: Channel dimension after fusion (default: 256)
        n_output_channels: Number of output channels (default: 1 for depth)
        use_cls_token: Whether CLS token is available (default: False)
    """

    def __init__(
        self,
        in_channels: List[int],
        fusion_channels: int = 256,
        n_output_channels: int = 1,
        use_cls_token: bool = False,
        **kwargs
    ):
        super().__init__()
        assert len(in_channels) == 4, "SDTHead expects exactly 4 feature layers"

        self.use_cls_token = use_cls_token
        self.fusion_channels = fusion_channels
        self.n_output_channels = n_output_channels

        self.weighted_fusion = WeightedFusion(in_channels, fusion_channels)
        self.detail_enhancer = SpatialDetailEnhancer(fusion_channels)

        self.upsample_1 = DySampleUpsamplerWrapper(fusion_channels, scale_factor=4, style='lp', groups=4, dyscope=True)
        self.refinement_1 = nn.Sequential(
            nn.Conv2d(fusion_channels, fusion_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(fusion_channels),
            nn.ReLU(inplace=True))

        self.upsample_2 = DySampleUpsamplerWrapper(fusion_channels, scale_factor=4, style='lp', groups=4, dyscope=True)
        self.refinement_2 = nn.Sequential(
            nn.Conv2d(fusion_channels, fusion_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(fusion_channels),
            nn.ReLU(inplace=True))

        self.output_conv = nn.Sequential(
            nn.Conv2d(fusion_channels, fusion_channels // 2, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(fusion_channels // 2, 32, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, n_output_channels, kernel_size=1))

    def forward(self, features: List) -> torch.Tensor:
        """
        Forward pass through SDTHead.

        Args:
            features: List of 4 feature entries, each either:
                - tuple (spatial_tensor [B,C,H,W], cls_token [B,C])
                - tensor [B,C,H,W]

        Returns:
            Depth prediction tensor [B, n_output_channels, H_out, W_out]
        """
        if isinstance(features[0], tuple):
            spatial_tensors = [f[0] for f in features]
            features_with_cls_token = features
        else:
            spatial_tensors = features
            features_with_cls_token = features

        B = spatial_tensors[0].shape[0]
        H_patches = spatial_tensors[0].shape[2]
        W_patches = spatial_tensors[0].shape[3]

        fused_tokens = self.weighted_fusion(features_with_cls_token)
        fused_spatial = fused_tokens.permute(0, 2, 1).contiguous().reshape(B, self.fusion_channels, H_patches, W_patches)
        enhanced_spatial = self.detail_enhancer(fused_spatial)

        x = self.upsample_1(enhanced_spatial, None)
        x = self.refinement_1(x)
        x = self.upsample_2(x, None)
        x = self.refinement_2(x)

        return self.output_conv(x)
